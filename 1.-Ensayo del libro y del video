## 1.- Introducción a la Inteligencia Artificial. Actividad 1,  Practica  1

### Ensayo del capitulo 1, 2, 26, 27, apartado  A  del  libro: Inteligencia  Artificial  un  enfoque  moderno.


**"Inteligencia Artificial: Un Enfoque Moderno" de Stuart J. Russell**


**Introducción:**

Se hara un ensayo sobre la Inteligencia Artificial un Enfoque Moderno. Libro realizado por Stuart J. Russell.
La IA es una de las ciencias más recientes. El trabajo comenzó poco después de la Segunda Guerra Mundial, y el nombre se acuñó en 1956. 
La IA se cita, junto a la biología molecular, como un campo en el que a la mayoría de científicos de otras disciplinas les gustaría trabajar.
Un estudiante de ciencias físicas puede pensar razonablemente que todas las buenas ideas han sido ya propuestas por Galileo, Newton, Einstein y otros. 
Por el contrario, la IA aún tiene flecos sin cerrar en los que podrían trabajar varios Einsteins a tiempo completo.
La IA abarca en la actualidad una gran variedad de subcampos, que van desde áreas de propósito general, como el aprendizaje y la percepción, a otras más específicas como el ajedrez, la demostración de teoremas matemáticos, la escritura de poesía y el diagnóstico de enfermedades.
La IA sintetiza y automatiza tareas intelectuales y es, por lo tanto, potencialmente relevante para cualquier ámbito de la actividad intelectual humana. 
En este sentido, es un campo genuinamente universal. El objetivo de este ensayo es analizar y evaluar los conceptos clave presentados en dichos capítulos de este extenso libro sobre la IA.

**Desarrollo:**

**Capítulo 1: Introducción a la Inteligencia Artificial**
En este primer capitulo, se aborda el tema de la inteligencia artificial (IA) y se destacan diferentes enfoques y definiciones relacionadas con este campo. 
Se presentan ocho definiciones de inteligencia artificial, divididas en dos grupos: aquellas que se centran en procesos mentales y razonamiento, y aquellas que se refieren a la conducta. 
Además, se establece una distinción entre medir el éxito en términos de fidelidad a la forma de actuar de los humanos y referirse a un concepto ideal de inteligencia llamado racionalidad, donde un sistema es considerado racional si realiza acciones "correctas" según su conocimiento.

Se menciona que a lo largo de la historia se han seguido cuatro enfoques en inteligencia artificial: sistemas que piensan como humanos, sistemas que piensan racionalmente, sistemas que actúan como humanos y sistemas que actúan racionalmente. 
Existe un enfrentamiento entre los enfoques centrados en los humanos y aquellos centrados en la racionalidad. El enfoque centrado en el comportamiento humano se considera una ciencia empírica, mientras que el enfoque racional implica una combinación de matemáticas e ingeniería.
Ambos enfoques han interactuado, a veces ayudándose mutuamente y otras veces ignorándose.
Se proporcionan ejemplos de definiciones de inteligencia artificial de diferentes autores y se destaca la diversidad de perspectivas en el campo, desde la automatización de actividades vinculadas con procesos de pensamiento humano hasta el estudio de las facultades mentales mediante el uso de modelos computacionales. 

Al igual se aborda el enfoque de la Prueba de Turing en el comportamiento humano como una medida operativa de la inteligencia artificial propuesta por Alan Turing en 1950. 
La prueba busca determinar la capacidad de un computador para imitar respuestas humanas al punto de que un evaluador no pueda distinguir entre las respuestas de una máquina y las de una persona. 
Para superar la prueba, se requiere que el computador posea habilidades en procesamiento de lenguaje natural, representación del conocimiento, razonamiento automático y aprendizaje automático.
La Prueba de Turing evita deliberadamente la interacción física directa y se centra en la capacidad de simular inteligencia. Se menciona la Prueba Global de Turing, que incluye elementos de percepción visual y manipulación física para evaluar aún más la inteligencia de la máquina. 
Se destaca que estas disciplinas, junto con las mencionadas anteriormente, abarcan gran parte del campo de la inteligencia artificial.
Se reconoce la relevancia continua de la Prueba de Turing después de 50 años, aunque los investigadores en inteligencia artificial han dedicado poco esfuerzo a evaluar sistemas con esta prueba, enfocándose más en los principios subyacentes de la inteligencia. 
La analogía se establece con la aviación, donde el éxito llegó al comprender los principios de la aerodinámica en lugar de imitar directamente a las aves. 
Se concluye que la búsqueda de la inteligencia artificial debe centrarse en comprender los principios en lugar de duplicar comportamientos específicos.

Se proporciona una breve historia de las disciplinas que han contribuido al desarrollo de la IA. La revisión histórica se organiza alrededor de preguntas filosóficas clave, sin pretender abarcar todas las preocupaciones de las disciplinas involucradas. 
Se destaca la importancia de avanzar en la IA como objetivo final de estas disciplinas. Se mencionan varias cuestiones filosóficas, desde el uso de reglas formales para extraer conclusiones válidas hasta la relación entre conocimiento y acción. 
Aristóteles, Leonardo da Vinci, y Thomas Hobbes son referenciados en el contexto de la historia del pensamiento relacionado con la inteligencia. 
Se aborda la distinción entre mente y materia, el dualismo versus materialismo, y la relación entre conocimiento y acción.
Se destaca la importancia del conocimiento empírico y se mencionan figuras como Francis Bacon, John Locke, y David Hume. Se explora el positivismo lógico, liderado por el Círculo de Viena, y se menciona la teoría de la confirmación de Carnap y Hempel como una de las primeras en representar la mente como un proceso computacional.
La relación entre conocimiento y acción se discute a través de la perspectiva de Aristóteles, y se señala la implementación posterior del algoritmo aristotélico por Newell y Simon en el contexto de la planificación regresiva. 
Además, se mencionan enfoques cuantitativos para la toma de decisiones propuestos por Antoine Arnauld y la idea del utilitarismo de John Stuart Mill.

Es importante abordar el papel de las matemáticas en el desarrollo de la inteligencia artificial, centrándonos en tres áreas fundamentales: lógica, computación y probabilidad.
1. **Lógica y Computación:**
   - La lógica formal se originó en la antigua Grecia, pero George Boole (1815-1864) y Gottlob Frege (1848-1925) contribuyeron al desarrollo matemático de la lógica proposicional y de primer orden.
   - David Hilbert (1862-1943) planteó el problema de decisión en 1900, preguntando si existe un algoritmo para determinar la validez de cualquier proposición lógica.
   - Kurt Gödel (1906-1978) demostró en 1931 los límites de la lógica de primer orden con su teorema de incompletitud, indicando la existencia de afirmaciones verdaderas no decidibles por algoritmos.
   - Alan Turing (1912-1954) contribuyó con la máquina de Turing y la tesis de Church-Turing, estableciendo la capacidad de una máquina para calcular cualquier función computable.
   - Se destacan conceptos como la no decidibilidad, computabilidad y la complejidad de problemas, especialmente con la introducción de la NP-completitud por Cook y Karp en la década de 1970.

2. **Problemas Intratables y Teoría de la NP-Completitud:**
   - Se discute la intratabilidad de problemas cuyo tiempo de resolución crece exponencialmente con el tamaño de los casos.
   - La teoría de la NP-completitud, propuesta por Cook y Karp, ofrece un método para reconocer problemas intratables.
   - A pesar del aumento en la velocidad de los computadores, la resolución eficiente de problemas intratables sigue siendo un desafío clave en la IA.

3. **Teoría de la Probabilidad:**
   - Gerolamo Cardano (1501-1576) introduce la probabilidad en el contexto de juegos de apuesta.
   - La probabilidad se convierte en una herramienta esencial en ciencias cuantitativas, tratando mediciones con incertidumbre.
   - Thomas Bayes (1702-1761) propone la regla de Bayes para actualizar probabilidades subjetivas a la luz de nuevas evidencias, sentando las bases para el análisis bayesiano en la IA moderna.

Es de suma importancia destacar cómo los límites y desafíos planteados en la lógica y la teoría de la computación han influido en la formulación de problemas y enfoques en la IA contemporánea.


La neurociencia, que abarca desde 1861 hasta el presente, se centra en comprender cómo procesa información el cerebro. 
A lo largo de la historia, se ha avanzado desde la creencia de que la mente estaba en el corazón hacia la aceptación de que el cerebro es la base de la conciencia.
1. **Descubrimientos Iniciales:**
   - Paul Broca (1824-1880) contribuyó al campo al estudiar la afasia en pacientes con daño cerebral en 1861, identificando el área de Broca en el hemisferio izquierdo como responsable del habla.
   - Camillo Golgi (1843-1926) desarrolló una técnica de coloración neuronal en 1873, permitiendo la observación de neuronas individuales.
   - Santiago Ramón y Cajal (1852-1934) utilizó esta técnica en sus estudios pioneros sobre la estructura neuronal.

2. **Avances Tecnológicos:**
   - En 1929, Hans Berger descubrió el electroencefalograma (EEG), permitiendo estudios sobre la actividad cerebral en individuos intactos.
   - Las imágenes de resonancia magnética funcional (IRMF) desde 1990 proporcionan imágenes detalladas de la actividad cerebral, avanzando en la comprensión de procesos cognitivos.

3. **Desafíos Actuales:**
   - A pesar de los avances, persisten desafíos en comprender la relación entre las áreas cerebrales, la plasticidad cerebral, y el almacenamiento de recuerdos individuales.
   - La complejidad del cerebro, que genera pensamiento, acción y conciencia, sigue siendo un misterio científico.

4. **Comparación con Computadores:**
   - Se destaca la diferencia entre cerebros y computadores digitales, resaltando que hay 1.000 veces más neuronas en un cerebro humano que puertas lógicas en la CPU de un computador estándar.
   - Aunque la ley de Moore predice la igualación en el número de neuronas y puertas lógicas alrededor de 2020, se enfatiza que las capacidades de procesamiento y paralelismo del cerebro siguen siendo superiores.

Es asombrosa la capacidad del cerebro para generar razonamiento, acción y conciencia. Tomando en cuenta la diferencia fundamental en las tareas realizadas por el cerebro y los computadores digitales.


En 1957, B. F. Skinner publicó "Verbal Behavior", una obra conductista sobre el aprendizaje del lenguaje. Sin embargo, Noam Chomsky criticó esta perspectiva en su revisión, señalando la incapacidad del conductismo para explicar la creatividad del lenguaje. 
Chomsky propuso una teoría basada en modelos sintácticos que abordaba esta limitación y permitía su programación. La lingüística moderna y la inteligencia artificial (IA) evolucionaron juntas en un campo híbrido llamado lingüística computacional o procesamiento del lenguaje natural. 
Aunque inicialmente se subestimó la complejidad del entendimiento del lenguaje, se reconoció que va más allá de la estructura de las oraciones, involucrando la comprensión del contexto y la materia bajo estudio.
En los años 60, la investigación en representación del conocimiento, especialmente en relación con el lenguaje y la búsqueda de información, estuvo influenciada por décadas de análisis filosófico del lenguaje.
La lingüística y la inteligencia artificial se entrelazaron, dando lugar a avances significativos en la comprensión y procesamiento del lenguaje.

En 1956, John McCarthy, junto a Marvin Minsky, Claude Shannon, y Nathaniel Rochester, organizó un taller en Dartmouth College, considerado el nacimiento oficial de la Inteligencia Artificial (IA). 
Aunque el taller no produjo avances notables, reunió a figuras importantes en el campo. Allen Newell y Herbert Simon del Carnegie Tech presentaron un programa llamado Teórico Lógico, capaz de razonamiento no numérico.
Después del taller, Newell y Simon desarrollaron un programa que demostraba teoremas matemáticos de Principia Matemática de Russell y Whitehead. 
Aunque el programa generó interés, el Journal of Symbolic Logic rechazó su artículo. A pesar de la falta de avances inmediatos, el taller estableció el término "Inteligencia Artificial" propuesto por McCarthy y conectó a las figuras clave del campo.
La IA se destacó por su objetivo de duplicar facultades humanas como la creatividad y el uso del lenguaje, a diferencia de otros campos como la teoría de control o la investigación operativa. 
La IA se consolidó como una rama de la informática, persiguiendo la construcción de máquinas para operar automáticamente en entornos complejos y cambiantes.



**Capítulo 2: Agentes Inteligentes**
Un agente es cualquier cosa capaz de percibir su medioambiente con la ayuda de sensores y actuar en ese medio utilizando actuadores.
En el contexto de agentes, ya sean humanos o robots, la percepción se refiere a la capacidad del agente para recibir entradas en cualquier momento. La secuencia de percepciones de un agente representa su historial completo de entradas recibidas. 
Se asume que cada agente puede percibir sus propias acciones, aunque no siempre sus efectos. La toma de decisiones de un agente en un momento dado depende de toda la secuencia de percepciones hasta ese instante.
La función del agente describe matemáticamente su comportamiento, proyectando percepciones en acciones. La función del agente es una descripción abstracta, mientras que el programa del agente es una implementación concreta que se ejecuta en la arquitectura del agente.
Se utiliza un ejemplo simple, el mundo de la aspiradora, para ilustrar estos conceptos. En este mundo, la aspiradora tiene dos ubicaciones y puede percibir la presencia de suciedad en cada ubicación.Una función de agente simple podría ser aspirar si la ubicación está sucia, de lo contrario, cambiar de ubicación.
La racionalidad de un agente en un momento dado se basa en cuatro factores: la medida de rendimiento que define el éxito, el conocimiento acumulado del entorno, las acciones disponibles para el agente y la secuencia de percepciones hasta el momento.
Un agente racional toma la acción que maximiza su medida de rendimiento, considerando la evidencia de la secuencia de percepciones y el conocimiento almacenado.
Tomando el ejemplo de un agente aspiradora que limpia una cuadrícula si está sucia y se mueve a la otra si no lo está, su racionalidad depende del contexto.
Si la medida de rendimiento premia con un punto por cada cuadrícula limpia en un período de tiempo específico a lo largo de 1,000 períodos, y el entorno se conoce a priori con ciertas restricciones, entonces el agente es racional. 
Sin embargo, en diferentes circunstancias, como cuando toda la suciedad se ha eliminado y la medida de rendimiento penaliza los movimientos innecesarios, el agente puede volverse irracional. Se exploran casos específicos y se diseñan agentes para abordar estas situaciones en ejercicios adicionales.

En el contexto de la racionalidad, es crucial distinguir entre racionalidad y omnisciencia. La omnisciencia implica conocer el resultado real de las acciones, lo cual es imposible en la realidad. 
Un ejemplo ilustrativo es el acto de cruzar una calle basado en la información disponible, aunque desconociendo eventos futuros.
Racionalidad no implica perfección, ya que se centra en maximizar el rendimiento esperado, no el resultado real. La definición de racionalidad propuesta no requiere omnisciencia; la elección racional se basa en la secuencia de percepciones hasta la fecha.
La recopilación de información y el aprendizaje son componentes esenciales de la racionalidad. Se aborda el concepto de autonomía, indicando que un agente racional debe ser capaz de aprender y ajustarse a su entorno, siendo autónomo en la toma de decisiones.
La autonomía completa puede no ser necesaria desde el principio, pero a medida que el agente acumula experiencia, su comportamiento se vuelve independiente del conocimiento inicial proporcionado por el diseñador. La incorporación del aprendizaje facilita el diseño de agentes racionales que pueden tener éxito en diversos entornos.

Las propiedades de los entornos de trabajo en inteligencia artificial pueden ser clasificadas en diversas dimensiones, lo que influye en el diseño y la implementación de los agentes. Estas dimensiones incluyen:

1. **Totalmente observable vs. parcialmente observable:**
   - Totalmente observable: El agente tiene acceso al estado completo del entorno en cada momento.
   - Parcialmente observable: El agente no tiene acceso completo al estado del entorno debido a ruido, sensores poco precisos, o limitaciones en la información recibida.

2. **Determinista vs. estocástico:**
   - Determinista: El siguiente estado del entorno está completamente determinado por el estado actual y las acciones del agente.
   - Estocástico: El siguiente estado tiene cierta incertidumbre, incluso si el entorno es observable y determinista.

3. **Episódico vs. secuencial:**
   - Episódico: La experiencia del agente se divide en episodios atómicos, donde cada episodio consiste en percepción y acción independientes de episodios anteriores.
   - Secuencial: Las decisiones actuales del agente afectan las decisiones futuras; las acciones están vinculadas a través del tiempo.

4. **Estático vs. dinámico:**
   - Estático: El entorno no cambia mientras el agente está tomando decisiones.
   - Dinámico: El entorno puede cambiar mientras el agente está deliberando sobre acciones.

5. **Discreto vs. continuo:**
   - Discreto: El entorno tiene un conjunto finito de estados distintos, percepciones y acciones.
   - Continuo: El entorno tiene estados, percepciones o acciones que pueden tomar valores en rangos continuos.

6. **Agente individual vs. multiagente:**
   - Agente individual: Un solo agente interactúa con el entorno.
   - Multiagente: Varios agentes pueden interactuar, ya sea de manera cooperativa o competitiva.

Los entornos reales a menudo son complejos y pueden tener propiedades de varias dimensiones, lo que hace que el diseño de agentes sea un desafío. La evaluación y experimentación en entornos simulados son esenciales para comprender el rendimiento de los agentes en contextos específicos.


**Capítulo 26: Aprendizaje Automático**













**Capítulo 27: Aprendizaje Profundo**
- Resumen del capítulo: Describe los principios del aprendizaje profundo según el autor.
- Análisis crítico: Examina cómo el aprendizaje profundo ha transformado el campo de la inteligencia artificial y sus desafíos.

**Apartado A: Agentes Racionales**
- Resumen del apartado: Presenta los elementos clave sobre agentes racionales según el libro.
- Análisis crítico: Analiza la noción de agentes racionales y su relevancia en el contexto de la inteligencia artificial moderna.

**Conclusiones:**
- Recapitula los puntos clave de cada capítulo y el apartado A.
- Ofrece una evaluación general del enfoque de Russell en la inteligencia artificial.
- Proporciona tu opinión sobre la eficacia de los conceptos presentados y su aplicabilidad en el campo actual.

**Bibliografía:**
- Incluye todas las referencias al libro "Inteligencia Artificial: Un Enfoque Moderno" de Stuart J. Russell.

